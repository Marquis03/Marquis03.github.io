<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models | 小嗷犬</title><meta name="keywords" content="大模型,论文笔记,微调"><meta name="author" content="小嗷犬"><meta name="copyright" content="小嗷犬"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"><meta name="application-name" content="【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"><meta property="og:url" content="https://blog.marquis.eu.org/posts/5301fcd1/index.html"><meta property="og:site_name" content="小嗷犬"><meta property="og:description" content="基本信息 标题: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models 作者: Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://i-blog.csdnimg.cn/direct/d8ae3ba314174e979eb534800bca9b6a.png"><meta property="article:author" content="小嗷犬"><meta property="article:tag" content="Python, 人工智能, 深度学习, 爬虫, 数据分析, 数据可视化, 小嗷犬, PWA"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i-blog.csdnimg.cn/direct/d8ae3ba314174e979eb534800bca9b6a.png"><meta name="description" content="基本信息 标题: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models 作者: Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://blog.marquis.eu.org/posts/5301fcd1/"><link rel="preconnect" href="//cdn.cbd.int"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="7oHto6XqE_zXYM7itwHoZTeb47PULzcAxenBeT1h-c4"><meta name="baidu-site-verification" content="codeva-izBOFPih2F"><meta name="msvalidate.01" content="390C5F1424AB7444F1B8E05DA02232BF"><link rel="manifest" href="/manifest.json"><meta name="msapplication-TileColor" content="var(--anzhiyu-main)"><link rel="mask-icon" href="/img/siteicon/apple-touch-icon.png" color="#5bbad5"><link rel="apple-touch-icon" sizes="180x180" href="/img/siteicon/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" sizes="180x180" href="/img/siteicon/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/img/siteicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/img/siteicon/favicon-16x16.png"><link rel="bookmark" href="/img/siteicon/apple-icon-180.png"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2048-2732.jpg" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2732-2048.jpg" media="(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1668-2388.jpg" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2388-1668.jpg" media="(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1536-2048.jpg" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2048-1536.jpg" media="(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1668-2224.jpg" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2224-1668.jpg" media="(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1620-2160.jpg" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2160-1620.jpg" media="(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1290-2796.jpg" media="(device-width: 430px) and (device-height: 932px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2796-1290.jpg" media="(device-width: 430px) and (device-height: 932px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1179-2556.jpg" media="(device-width: 393px) and (device-height: 852px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2556-1179.jpg" media="(device-width: 393px) and (device-height: 852px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1284-2778.jpg" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2778-1284.jpg" media="(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1170-2532.jpg" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2532-1170.jpg" media="(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1125-2436.jpg" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2436-1125.jpg" media="(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1242-2688.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2688-1242.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-828-1792.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1792-828.jpg" media="(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1242-2208.jpg" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-2208-1242.jpg" media="(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-750-1334.jpg" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1334-750.jpg" media="(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-640-1136.jpg" media="(device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"><link rel="apple-touch-startup-image" href="/img/siteicon/apple-splash-1136-640.jpg" media="(device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?368c140290209ced69f65d67bf17e5fd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script>const GLOBAL_CONFIG={linkPageTop:{enable:!0,title:"与数百名博主无限进步",addFriendPlaceholder:"昵称（请勿包含博客等字样）：\n网站地址（要求博客地址，请勿提交个人主页）：\n头像图片url（请提供尽可能清晰的图片，我会上传到我自己的图床）：\n描述：\n站点截图（可选）：\n"},peoplecanvas:void 0,postHeadAiDescription:{enable:!0,gptName:"小嗷犬",mode:"local",switchBtn:!1,btnLink:"https://afdian.net/item/886a79d4db6711eda42a52540025c377",randomNum:3,basicWordCount:1e3,key:"xxxx",Referer:"https://xx.xx/"},diytitle:{enable:!0,leaveTitle:"w(ﾟДﾟ)w 不要走！再看看嘛！",backTitle:"♪(^∇^*)欢迎肥来！"},LA51:{enable:!0,ck:"JwZrZwQa4gtWzwsr",LingQueMonitorID:"JwcZGiBSGG3rSvij"},greetingBox:{enable:!0,default:"晚上好👋",list:[{greeting:"晚安😴",startTime:0,endTime:5},{greeting:"早上好鸭👋, 祝你一天好心情！",startTime:6,endTime:9},{greeting:"上午好👋, 状态很好，鼓励一下～",startTime:10,endTime:10},{greeting:"11点多啦, 在坚持一下就吃饭啦～",startTime:11,endTime:11},{greeting:"午安👋, 宝贝",startTime:12,endTime:14},{greeting:"🌈充实的一天辛苦啦！",startTime:14,endTime:18},{greeting:"19点喽, 奖励一顿丰盛的大餐吧🍔。",startTime:19,endTime:19},{greeting:"晚上好👋, 在属于自己的时间好好放松😌~",startTime:20,endTime:24}]},twikooEnvId:"https://twikoo.marquis.eu.org/",commentBarrageConfig:{enable:!0,maxBarrage:1,barrageTime:4e3,accessToken:"",mailMd5:"29ae50b4bc2db4bc06b67953cffcde91"},root:"/",preloader:{source:3},friends_vue_info:void 0,navMusic:!0,mainTone:{mode:"api",api:"https://img2color.marquis.eu.org/api?img=",cover_change:!0},authorStatus:{skills:["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},algolia:{appId:"DVYRTF6D9Z",apiKey:"f2a8d629317821324cdf3875087d3364",indexName:"blog",hits:{per_page:6},languages:{input_placeholder:"输入关键词后按下回车查找",hits_empty:"找不到您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，用时 ${time} 毫秒"}},localSearch:void 0,translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简",rightMenuMsgToTraditionalChinese:"转为繁体",rightMenuMsgToSimplifiedChinese:"转为简体"},noticeOutdate:{limitDay:365,position:"top",messagePrev:"这篇文章距离上次更新已经过去了",messageNext:"天，其中的某些内容可能不再适用了，请谨慎阅读。"},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:400},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,simplehomepage:!0,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{copy:!0,copyrightEbable:!0,limitCount:50,languages:{author:"作者: 小嗷犬",link:"链接: ",source:"来源: 小嗷犬",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。",copySuccess:"复制成功，复制和转载请标注本文地址"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#425AEF",bgDark:"#1f1f1f",position:"top-center"},source:{justifiedGallery:{js:"https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js",css:"https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css"}},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,shortcutKey:void 0,autoDarkmode:!0}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={configTitle:"小嗷犬",title:"【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",postAI:"本文介绍了一种名为LongLoRA的高效微调方法，旨在扩展大型语言模型（LLM）的上下文长度，同时降低计算成本。LongLoRA通过结合移位稀疏注意力（S2-Attn）和改进的LoRA技术，实现了在训练过程中对长上下文的有效处理。该方法不仅在保持模型性能的同时显著减少了计算资源的消耗，还与现有的技术如Flash-Attention2兼容，展示了在Llama2模型上的强大实证结果。通过将Llama2 7B的上下文从4k扩展到100k，或将70B模型扩展到32k，LongLoRA证明了其在长上下文任务中的有效性和实用性。",pageFillDescription:"基本信息, 摘要, 简介, LongLoRA, 背景, Transformer, Low-rank Adaptation, Shifted Sparse Attention, Pilot Study, Consistency to Full Attention, Easy Implementation, Improved LoRA for Long Context, 实验, 主实验, 消融实验, 总结基本信息标题作者发表摘要我们提出了一种高效的微调方法它通过有限的计算成本扩展了预训练大型语言模型的上下文大小通常使用长上下文大小训练在计算上非常昂贵需要大量的训练时间和资源例如在个上下文长度的训练中自注意力层的计算成本是个上下文长度的倍在本文中我们从两个方面加速了上下文扩展一方面尽管在推理过程中需要密集的全局注意力但通过稀疏局部注意力可以有效地进行模型微调提出的移位稀疏注意力有效地实现了上下文扩展与使用标准注意力微调具有相似的性能同时实现了显著的计算节省特别是它可以在训练中仅用两行代码实现而在推理中是可选的另一方面我们重新审视了参数高效的上下文扩展微调机制值得注意的是我们发现在可训练嵌入和归一化的前提下对于上下文扩展效果良好将这种改进的与相结合在模型从到的各种任务上展示了强大的实证结果将的上下文从扩展到或将扩展到在单个机器上完成在保持原始架构的同时扩展了模型的上下文并且与大多数现有技术兼容如此外我们还使用和我们的长指令遵循数据集进行了监督微调我们所有的代码模型数据集和演示代码都可在上找到简介缩小了传统和全量微调之间的精度差距同时保持了比全量微调低倍的内存成本此外通过将的训练速度提高了高达倍使用和的第二阶段进行微调并在测试集上评估了困惑度的示意图涉及三个步骤首先它将特征沿头部维度分为两个部分其次其中一个部分中的向右移动了组大小的一半第三我们将分成组并将它们重塑为批量维度在我们的模型中注意力仅在每组中计算而信息通过移动在组之间流动移动可能会引入潜在的信息泄露但通过在注意力掩码上进行微小修改可以轻松防止背景大型语言模型通常是基于构建的例如以为例一个模型由一个嵌入输入层和若干解码器层组成每个解码器层包含一个自注意力模块它通过带有权重矩阵的线性投影层将输入特征映射为一组查询键和值给定它计算输出输出随后通过一个权重矩阵的线性层进行投影接着是多层感知机层在自注意力模块之前和之后会应用层归一化所有解码器层完成后还会进行一次最终归一化对于较长的序列自注意力在计算成本方面表现出困难其计算复杂度与序列长度成平方关系这大幅减慢了训练过程并增加了内存的使用成本假设预训练模型中的权重更新在适配期间具有较低的内在秩对于一个预训练权重矩阵它通过低秩分解来更新其中秩在训练期间被冻结没有梯度更新而和是可训练的这就是训练比完全微调更高效的原因在结构中仅适配注意力权重并冻结所有其他层包括和归一化层这种方式简单且参数高效然而我们通过实验证明仅在注意力权重中的低秩适配并不能很好地适用于长上下文扩展任务标准的自注意力计算成本为这使得长序列上的具有高内存成本和低速为了在训练期间避免这一问题我们提出了移位稀疏注意力如图所示接下来我们将进行一项初步研究并逐步解释我们的设计在表中我们建立了一个标准基线该基线经过完整注意力和微调训练和测试在各种上下文长度下表现出一致的良好质量第一次试验是使用短注意力进行训练仅模式如图所示正如我们所知在长上下文中高昂的成本主要来自自注意力模块因此在这次试验中由于输入很长我们在自注意力中将其分为几个组例如模型在训练和测试阶段都以个作为输入但在每个组中进行自注意力操作组大小为组数为这种模式效率很高但在非常长的上下文中仍然不起作用如表所示随着上下文长度的增加困惑度变大其背后的原因是没有不同组之间的信息交换为了引入组之间的通信我们包括了一个移位模式如图所示我们在半注意力头中将组分区移位半个组大小以总体个上下文长度为例在模式中第一组从第个到第个进行自注意力在模式中组分区移位第一个注意力组从第个开始到第个结束而前个和最后个属于同一组我们在每个半自注意力头中分别使用模式和模式这种方式不会增加额外的计算成本但能够实现不同组之间的信息流我们在表中展示了它接近标准注意力基线的结果现有的高效注意力设计也可以提高长上下文的效率然而大多数这些设计并不适合长上下文微调因为这些从头开始训练的与预训练中使用的标准全注意力存在差距在表中我们展示了不仅能够实现高效的微调还支持全注意力测试尽管其他注意力机制也可以用于长上下文微调但模型必须使用微调期间使用的注意力进行测试移位防止了模型对特定注意力模式的过度拟合易于实现它仅涉及两个步骤在半注意力头中移位将特征从维度转置到批次维度两行代码就足够了我们在算法中提供了一个风格的代码示例是一种高效且流行的将适应其他数据集的方法与全微调相比它节省了大量的可训练参数和内存成本然而将从短上下文长度适应到长上下文长度并不容易我们观察到与全微调之间存在明显的差距如表所示随着目标上下文长度的增加与全微调之间的差距逐渐增大并且具有更大秩的无法缩小这个差距为了弥合这一差距我们为训练打开了嵌入层和归一化层如表所示它们占用的参数有限但对长上下文适应有显著效果特别是对于归一化层参数在整个中仅占在实验中我们将这种改进版的称为实验主实验消融实验总结在这项工作中我们提出了它能够高效地扩展的上下文长度使其显著更大与标准全微调相比具有更低的内存成本和训练时间同时精度损失最小在架构层面我们提出了用于在训练过程中近似标准自注意力模式易于实现仅需两行代码此外通过训练的模型在推理过程中保留了原始的标准注意力架构使得大多数现有基础设施和优化可以重用在训练层面我们通过可训练归一化和嵌入弥合了与全微调之间的差距我们的方法可以将扩展到上下文长度将模型扩展到上下文长度在单个机器上实现我们还提出了一个长指令遵循数据集并使用进行了监督微调我们相信是一种通用方法可以与更多类型的和位置编码兼容我们计划在未来工作中调查这些问题",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-06-01 16:31:02",postMainColor:"#ed7d31"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:(e,t,a)=>{if(0===a)return;const o={value:t,expiry:Date.now()+864e5*a};localStorage.setItem(e,JSON.stringify(o))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const a=JSON.parse(t);if(!(Date.now()>a.expiry))return a.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise(((a,o)=>{const c=document.createElement("script");c.src=e,c.async=!0,c.onerror=o,c.onload=c.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(c.onload=c.onreadystatechange=null,a())},Object.keys(t).forEach((e=>{c.setAttribute(e,t[e])})),document.head.appendChild(c)})),e.getCSS=(e,t=!1)=>new Promise(((a,o)=>{const c=document.createElement("link");c.rel="stylesheet",c.href=e,t&&(c.id=t),c.onerror=o,c.onload=c.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(c.onload=c.onreadystatechange=null,a())},document.head.appendChild(c)})),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#18171d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#f7f9fe")};const t=saveToLocal.get("theme"),a=window.matchMedia("(prefers-color-scheme: dark)").matches,o=window.matchMedia("(prefers-color-scheme: light)").matches,c=window.matchMedia("(prefers-color-scheme: no-preference)").matches,n=!a&&!o&&!c;if(void 0===t){if(o)activateLightMode();else if(a)activateDarkMode();else if(c||n){const e=(new Date).getHours();e<=6||e>=18?activateDarkMode():activateLightMode()}window.matchMedia("(prefers-color-scheme: dark)").addListener((e=>{void 0===saveToLocal.get("theme")&&(e.matches?activateDarkMode():activateLightMode())}))}else"light"===t?activateLightMode():activateDarkMode();const d=saveToLocal.get("aside-status");void 0!==d&&("hide"===d?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="referrer" content="never"><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/highlight-one-dark.css"><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link rel="alternate" href="/atom.xml" title="小嗷犬" type="application/atom+xml"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="/img/avatar/2.png"><div class="loading-image-dot"></div></div></div><script>const preloader={endLoading:()=>{document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",(()=>{preloader.endLoading()})),setTimeout((function(){preloader.endLoading()}),1e4),document.addEventListener("pjax:send",(()=>{preloader.initLoading()})),document.addEventListener("pjax:complete",(()=>{preloader.endLoading()}))</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"><script async src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://marquis.eu.org/" title="个人主页"><img class="back-menu-item-icon" src="/img/avatar/1.png" alt="个人主页"><span class="back-menu-item-text">个人主页</span></a><a class="back-menu-item" href="https://blog.marquis.eu.org/" title="个人博客"><img class="back-menu-item-icon" src="/img/avatar/2.png" alt="个人博客"><span class="back-menu-item-text">个人博客</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">小嗷犬</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size:.9em"></i> <span>归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size:.9em"></i> <span>分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size:.9em"></i> <span>标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/charts/"><i class="fas fa-chart-bar faa-tada"></i> <span>统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=8947107806&amp;server=tencent"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size:.9em"></i> <span>音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size:.9em"></i> <span>相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/favorites/"><i class="fas fa-star faa-tada"></i> <span>收藏夹</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/equipment/"><i class="anzhiyufont anzhiyu-icon-dice-d20 faa-tada" style="font-size:.9em"></i> <span>装备栏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size:.9em"></i> <span>友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size:.9em"></i> <span>留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size:.9em"></i> <span>关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size:.9em"></i> <span>闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size:.9em"></i> <span>随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i> <span>搜索</span></a></div><input id="center-console" type="checkbox"><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole()"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="/img/reward/wechat.png" target="_blank"><img class="post-qr-code-img" alt="WeChat Pay" src="/img/reward/wechat.png"></a><div class="post-qr-code-desc">WeChat Pay</div></li><li class="reward-item"><a href="/img/reward/alipay.png" target="_blank"><img class="post-qr-code-img" alt="Alipay" src="/img/reward/alipay.png"></a><div class="post-qr-code-desc">Alipay</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title">最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/C/" style="font-size:1.05rem">C<sup>1</sup></a><a href="/tags/CPP/" style="font-size:1.05rem">CPP<sup>5</sup></a><a href="/tags/LaTeX/" style="font-size:1.05rem">LaTeX<sup>4</sup></a><a href="/tags/Linux/" style="font-size:1.05rem">Linux<sup>1</sup></a><a href="/tags/MATLAB/" style="font-size:1.05rem;font-weight:500;color:var(--anzhiyu-lighttext)">MATLAB<sup>28</sup></a><a href="/tags/Markdown/" style="font-size:1.05rem">Markdown<sup>5</sup></a><a href="/tags/Python/" style="font-size:1.05rem;font-weight:500;color:var(--anzhiyu-lighttext)">Python<sup>43</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size:1.05rem">前端<sup>1</sup></a><a href="/tags/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" style="font-size:1.05rem">动作识别<sup>2</sup></a><a href="/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/" style="font-size:1.05rem">多任务<sup>1</sup></a><a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size:1.05rem">多模态<sup>41</sup></a><a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="font-size:1.05rem">大模型<sup>44</sup></a><a href="/tags/%E5%B0%91%E6%A0%B7%E6%9C%AC/" style="font-size:1.05rem">少样本<sup>4</sup></a><a href="/tags/%E5%BE%AE%E8%B0%83/" style="font-size:1.05rem">微调<sup>11</sup></a><a href="/tags/%E6%89%8B%E8%AF%AD%E7%BF%BB%E8%AF%91/" style="font-size:1.05rem">手语翻译<sup>21</sup></a><a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size:1.05rem">操作系统<sup>3</sup></a><a href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" style="font-size:1.05rem">数学建模<sup>20</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size:1.05rem">数据分析<sup>5</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/" style="font-size:1.05rem">数据可视化<sup>2</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size:1.05rem">机器学习<sup>8</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size:1.05rem">深度学习<sup>19</sup></a><a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size:1.05rem">爬虫<sup>5</sup></a><a href="/tags/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" style="font-size:1.05rem">程序设计<sup>11</sup></a><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size:1.05rem">论文笔记<sup>66</sup></a></div></div><hr></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多"><i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">21</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">十一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">22</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">14</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url">学习笔记</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" tabindex="-1" itemprop="url"><span><i class="anzhiyufont anzhiyu-icon-hashtag"></i> 大模型</span></a><a class="article-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" tabindex="-1" itemprop="url"><span><i class="anzhiyufont anzhiyu-icon-hashtag"></i> 论文笔记</span></a><a class="article-meta__tags" href="/tags/%E5%BE%AE%E8%B0%83/" tabindex="-1" itemprop="url"><span><i class="anzhiyufont anzhiyu-icon-hashtag"></i> 微调</span></a></span></div></div><h1 class="post-title" itemprop="name headline">【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-01-05T13:00:39.000Z" title="发表于 2025-01-05 21:00:39">2025-01-05</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-06-01T08:31:02.377Z" title="更新于 2025-06-01 16:31:02">2025-06-01</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">2.6k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>8分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" data-flag-title="【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator"></span><span class="post-meta-position" title="作者IP属地为成都"><i class="anzhiyufont anzhiyu-icon-location-dot"></i> 成都</span><span class="post-meta-separator"></span><span class="post-meta-commentcount"><i class="anzhiyufont anzhiyu-icon-comments post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/5301fcd1/#post-comment" tabindex="-1"><span id="twikoo-count"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></a></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s58 18 88 18 58-18 88-18 58 18 88 18v44h-352Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://i-blog.csdnimg.cn/direct/d8ae3ba314174e979eb534800bca9b6a.png"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><div class="post-ai-description"><div class="ai-title"><i class="anzhiyufont anzhiyu-icon-bilibili"></i><div class="ai-title-text">AI-摘要</div><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i><i class="anzhiyufont anzhiyu-icon-circle-dot" title="朗读摘要"></i><div id="ai-tag">小嗷犬 GPT</div></div><div class="ai-explanation">AI初始化中...</div><div class="ai-btn-box"><div class="ai-btn-item">介绍自己 🙈</div><div class="ai-btn-item">生成本文简介 👋</div><div class="ai-btn-item">推荐相关文章 📖</div><div class="ai-btn-item">前往主页 🏠</div><div class="ai-btn-item" id="go-tianli-blog">前往爱发电购买</div></div><script data-pjax src="/js/anzhiyu/ai_abstract.js"></script></div><article class="post-content" id="article-container" itemscope itemtype="https://blog.marquis.eu.org/posts/5301fcd1/"><header><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url">学习笔记</a><a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" tabindex="-1" itemprop="url">大模型</a><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" tabindex="-1" itemprop="url">论文笔记</a><a href="/tags/%E5%BE%AE%E8%B0%83/" tabindex="-1" itemprop="url">微调</a><h1 id="CrawlerTitle" itemprop="name headline">【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">小嗷犬</span><time itemprop="dateCreated datePublished" datetime="2025-01-05T13:00:39.000Z" title="发表于 2025-01-05 21:00:39">2025-01-05</time><time itemprop="dateCreated datePublished" datetime="2025-06-01T08:31:02.377Z" title="更新于 2025-06-01 16:31:02">2025-06-01</time></header><h2 id="基本信息">基本信息</h2><p><strong>标题</strong>: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models<br><strong>作者</strong>: Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia<br><strong>发表</strong>: ICLR 2024<br><strong>arXiv</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.12307">https://arxiv.org/abs/2309.12307</a></p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/dccb0a7240fd4360a4d27cf078c5b67d.png" alt="基本信息"></p><h3 id="摘要">摘要</h3><p>我们提出了<strong>LongLoRA</strong>，一种高效的微调方法，它通过有限的计算成本扩展了预训练大型语言模型（LLM）的上下文大小。</p><p>通常，使用长上下文大小训练LLM在计算上非常昂贵，需要大量的训练时间和GPU资源。例如，在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8192</mn></mrow><annotation encoding="application/x-tex">8192</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">8192</span></span></span></span> 个上下文长度的训练中，自注意力层的计算成本是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2048</mn></mrow><annotation encoding="application/x-tex">2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">2048</span></span></span></span> 个上下文长度的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn></mrow><annotation encoding="application/x-tex">16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6444em"></span><span class="mord">16</span></span></span></span> 倍。</p><p>在本文中，我们从两个方面加速了LLM上下文扩展。</p><p>一方面，尽管在推理过程中需要密集的全局注意力，但通过稀疏局部注意力可以有效地进行模型微调。提出的移位稀疏注意力（S2-Attn）有效地实现了上下文扩展，与使用标准注意力微调具有相似的性能，同时实现了显著的计算节省。特别是，它可以在训练中仅用两行代码实现，而在推理中是可选的。</p><p>另一方面，我们重新审视了参数高效的上下文扩展微调机制。值得注意的是，我们发现LoRA在可训练嵌入和归一化的前提下，对于上下文扩展效果良好。LongLoRA将这种改进的LoRA与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn相结合。</p><p>LongLoRA在Llama2模型（从7B/13B到70B）的各种任务上展示了强大的实证结果。LongLoRA将Llama2 7B的上下文从4k扩展到100k，或将Llama2 70B扩展到32k，在单个8×A100机器上完成。</p><p>LongLoRA在保持原始架构的同时扩展了模型的上下文，并且与大多数现有技术兼容，如Flash-Attention2。</p><p>此外，我们还使用LongLoRA和我们的长指令遵循LongAlpaca数据集进行了监督微调。</p><p>我们所有的代码、模型、数据集和演示代码都可在<a target="_blank" rel="noopener" href="https://github.com/dvlab-research/LongLoRA">github.com/dvlab-research/LongLoRA</a>上找到。</p><h2 id="简介">简介</h2><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/bbc60bb8c1f5427aaaecb0cd13a10b0a.png" alt="LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning, while still maintaining up to 1.8× lower memory cost than full fine-tuning. Furthermore, LongLoRA improves the training speed of LoRA by up to 1.8× with S2-Attn. Llama2-7B are fine-tuned to various context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage 2 and evaluated on the proof-pile (Azerbayev et al., 2022) test set in perplexity."></p><p>LongLoRA缩小了传统LoRA和全量微调之间的精度差距，同时保持了比全量微调低1.8倍的内存成本。此外，LongLoRA通过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn 将LoRA的训练速度提高了高达1.8倍。Llama2-7B使用Flash-Attention2和DeepSpeed的第二阶段进行微调，并在 proof-pile 测试集上评估了困惑度。</p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/6bead6c0b72943988f108999f028f854.png" alt="Illustration of S2-Attn. It involves three steps. First, it splits features along the head dimension into two chunks. Second, tokens in one of the chunks are shifted by half of the group size. Third, we split tokens into groups and reshape them into batch dimensions. Attention only computes in each group in ours while the information flows between groups via shifting. Potential information leakage might be introduced by shifting, while this is easy to prevent via a small modification on the attention mask. We ablate this in the variant 2 in Section B.3 in the appendix."></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn的示意图涉及三个步骤。首先，它将特征沿头部维度分为两个部分。其次，其中一个部分中的token向右移动了组大小的一半。第三，我们将token分成组，并将它们重塑为批量维度。在我们的模型中，注意力仅在每组中计算，而信息通过移动在组之间流动。移动可能会引入潜在的信息泄露，但通过在注意力掩码上进行微小修改可以轻松防止。</p><h2 id="LongLoRA">LongLoRA</h2><h3 id="背景">背景</h3><h4 id="Transformer">Transformer</h4><p>大型语言模型（LLMs）通常是基于 Transformer 构建的。例如，以 Llama2 为例，一个 LLM 模型由一个嵌入输入层和若干解码器层组成。每个解码器层包含一个自注意力模块。它通过带有权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>W</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>v</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{W_q, W_k, W_v\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span> 的线性投影层将输入特征映射为一组查询、键和值<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{q, k, v\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:.03588em">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="mclose">}</span></span></span></span>。给定<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{q, k, v\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:.03588em">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="mclose">}</span></span></span></span>，它计算输出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">o</span></span></span></span>：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>q</mi><msup><mi>k</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">o = \text{softmax}(qk^T)v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">o</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-.25em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.03588em">q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:.03588em">v</span></span></span></span></span></p><p>输出随后通过一个权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">W_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 的线性层进行投影，接着是多层感知机（MLP）层。在自注意力模块之前和之后，会应用层归一化。所有解码器层完成后，还会进行一次最终归一化。</p><p>对于较长的序列，自注意力在计算成本方面表现出困难，其计算复杂度与序列长度成平方关系。这大幅减慢了训练过程，并增加了 GPU 内存的使用成本。</p><h4 id="Low-rank-Adaptation">Low-rank Adaptation</h4><p>LoRA假设预训练模型中的权重更新在适配期间具有较低的内在秩（intrinsic rank）。对于一个预训练权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{d \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7224em;vertical-align:-.0391em"></span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span>，它通过低秩分解<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>W</mi><mo>=</mo><mi>W</mi><mo>+</mo><mi>B</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">W + \Delta W = W + BA</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7667em;vertical-align:-.0833em"></span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.7667em;vertical-align:-.0833em"></span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="mord mathnormal">A</span></span></span></span> 来更新，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">B \in \mathbb{R}^{d \times r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7224em;vertical-align:-.0391em"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{r \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7224em;vertical-align:-.0391em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8491em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span>。秩<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>≪</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>d</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r \ll \min(d, k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="mclose">)</span></span></span></span>。在训练期间，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">W</span></span></span></span> 被冻结（没有梯度更新），而<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal">A</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span></span></span></span> 是可训练的。这就是 LoRA 训练比完全微调更高效的原因。</p><p>在 Transformer 结构中，LoRA 仅适配注意力权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>W</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>v</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>o</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{W_q, W_k, W_v, W_o\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-.2861em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1514em"><span style="top:-2.55em;margin-left:-.1389em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>，并冻结所有其他层，包括 MLP 和归一化层。这种方式简单且参数高效。然而，我们通过实验证明，仅在注意力权重中的低秩适配并不能很好地适用于长上下文扩展任务。</p><h3 id="Shifted-Sparse-Attention">Shifted Sparse Attention</h3><p>标准的自注意力计算成本为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-.25em"></span><span class="mord mathcal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，这使得长序列上的LLM具有高内存成本和低速。为了在训练期间避免这一问题，我们提出了移位稀疏注意力（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn），如图2所示。接下来，我们将进行一项初步研究，并逐步解释我们的设计。</p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/d8ae3ba314174e979eb534800bca9b6a.png" alt="Overview of LongLoRA. We introduce Shifted Sparse Attention (S2-Attn) during finetuning. The trained model retains original standard self-attention at inference time. In addition to training LoRA weights in linear layers, LongLoRA further makes embedding and normalization layers trainable. This extension is pivotal for context extension, and only introduces a minimal number of additional trainable parameters."></p><h4 id="Pilot-Study">Pilot Study</h4><p>在表1中，我们建立了一个标准基线，该基线经过完整注意力和微调训练和测试，在各种上下文长度下表现出一致的良好质量。第一次试验是使用短注意力进行训练，仅模式1如图2所示。正如我们所知，在长上下文中，高昂的成本主要来自自注意力模块。因此，在这次试验中，由于输入很长，我们在自注意力中将其分为几个组。例如，模型在训练和测试阶段都以8192个token作为输入，但在每个组中进行自注意力操作，组大小为2048，组数为4。这种模式效率很高，但在非常长的上下文中仍然不起作用，如表1所示。随着上下文长度的增加，困惑度变大。其背后的原因是没有不同组之间的信息交换。</p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/fa355f56bf0d4d2891fa233b6cb6c6a4.png" alt="Effectiveness of S2-Attn under different context lengths"></p><p>为了引入组之间的通信，我们包括了一个移位模式，如图2所示。我们在半注意力头中将组分区移位半个组大小。以总体8192个上下文长度为例，在模式1中，第一组从第1个到第2048个token进行自注意力。在模式2中，组分区移位1024。第一个注意力组从第1025个开始到第3072个token结束，而前1024个和最后1024个token属于同一组。我们在每个半自注意力头中分别使用模式1和模式2。这种方式不会增加额外的计算成本，但能够实现不同组之间的信息流。我们在表1中展示了它接近标准注意力基线的结果。</p><h4 id="Consistency-to-Full-Attention">Consistency to Full Attention</h4><p>现有的高效注意力设计也可以提高长上下文LLM的效率。然而，大多数这些设计并不适合长上下文微调。因为这些从头开始训练的Transformer与预训练中使用的标准全注意力存在差距。在表6中，我们展示了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn 不仅能够实现高效的微调，还支持全注意力测试。尽管其他注意力机制也可以用于长上下文微调，但模型必须使用微调期间使用的注意力进行测试。移位防止了模型对特定注意力模式的过度拟合。</p><h4 id="Easy-Implementation">Easy Implementation</h4><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn易于实现。它仅涉及两个步骤：</p><ol><li>在半注意力头中移位token；</li><li>将特征从token维度转置到批次维度。</li></ol><p>两行代码就足够了。我们在算法1中提供了一个PyTorch风格的代码示例。</p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/5d410f29dc94494280c5087034bb56a4.png" alt="Algorithm 1: Pseudocode of S2-Attn in PyTorch-like style."></p><h3 id="Improved-LoRA-for-Long-Context">Improved LoRA for Long Context</h3><p>LoRA是一种高效且流行的将LLMs适应其他数据集的方法。与全微调相比，它节省了大量的可训练参数和内存成本。然而，将LLMs从短上下文长度适应到长上下文长度并不容易。我们观察到LoRA与全微调之间存在明显的差距。如表2所示，随着目标上下文长度的增加，LoRA与全微调之间的差距逐渐增大。并且，具有更大秩的LoRA无法缩小这个差距。</p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/69864c286600413b95692673f99e9f28.png" alt="Finetuning normalization and embedding layers is crucial for low-rank long-context adaptation"></p><p>为了弥合这一差距，我们为训练打开了嵌入层和归一化层。如表2所示，它们占用的参数有限，但对长上下文适应有显著效果。特别是对于归一化层，参数在整个Llama2 7B中仅占0.004%。在实验中，我们将这种改进版的LoRA称为LoRA+。</p><h2 id="实验">实验</h2><h3 id="主实验">主实验</h3><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/5ed2ceaabdb94582bf33a296b673fce7.png" alt="Perplexity evaluation on proof-pile (Rae et al., 2020) test split"></p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/c98604130ca6492387d1a1dc227c7641.png" alt="Maximum context length that we can fine-tune for various model sizes on a single 8× A100 machine"></p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/fa6a973dcf504a509529feab363603f0.png" alt="Topic retrieval evaluation with LongChat (Li et al., 2023)"></p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/ff23eeba43e241ccbbd55ee949348921.png" alt="Accuracy comparison on passkey retrieval between Llama2 7B and our 7B model fine-tuned on 32768 context length"></p><h3 id="消融实验">消融实验</h3><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/7e235d867c804bc391a1b4ca3ca1a751.png" alt="Ablation on fine-tuning steps in both full fine-tuning and LoRA+"></p><p><img src="/img/loading.gif" onerror="this.onerror=null,this.src=&quot;/img/friend_404.gif&quot;" data-lazy-src="https://i-blog.csdnimg.cn/direct/bc73174058064cc3ac00ce5f7b66b1ba.png" alt="Comparisons among S2-Attn and alternative attention patterns during fine-tuning"></p><h2 id="总结">总结</h2><p>在这项工作中，我们提出了LongLoRA，它能够高效地扩展LLMs的上下文长度，使其显著更大。</p><p>与标准全微调相比，LongLoRA具有更低的GPU内存成本和训练时间，同时精度损失最小。</p><p>在架构层面，我们提出了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn，用于在训练过程中近似标准自注意力模式。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn 易于实现，仅需两行代码。</p><p>此外，通过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-Attn 训练的模型在推理过程中保留了原始的标准注意力架构，使得大多数现有基础设施和优化可以重用。</p><p>在训练层面，我们通过可训练归一化和嵌入弥合了LoRA与全微调之间的差距。</p><p>我们的方法可以将Llama2 7B扩展到100k上下文长度，将70B模型扩展到32k上下文长度，在单个8×A100机器上实现。</p><p>我们还提出了一个长指令遵循数据集LongAlpaca，并使用LongLoRA进行了监督微调。</p><p>我们相信LongLoRA是一种通用方法，可以与更多类型的LLMs和位置编码兼容。</p><p>我们计划在未来工作中调查这些问题。</p></article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src="/img/avatar/2.png" title="头像" alt="头像"><img class="post-copyright__author_img_front" src="/img/avatar/2.png" title="头像" alt="头像"></a><div class="post-copyright__author_name">小嗷犬</div><div class="post-copyright__author_desc">分享技术，记录生活</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://blog.marquis.eu.org/posts/5301fcd1/">原创</a><a class="post-copyright-title"><span onclick='rm.copyPageUrl("https://blog.marquis.eu.org/posts/5301fcd1/")'>【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="/img/reward/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/reward/wechat.png" alt="WeChat Pay"></a><div class="post-qr-code-desc">WeChat Pay</div></li><li class="reward-item"><a href="/img/reward/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/reward/alipay.png" alt="Alipay"></a><div class="post-qr-code-desc">Alipay</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display:none"></div><div class="reward-link mode"><a class="reward-link-button" href="/operate/"><i class="anzhiyufont anzhiyu-icon-plant-fill"></i>运营模式与责任</a></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://blog.marquis.eu.org/posts/5301fcd1/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models&amp;url=https://blog.marquis.eu.org/posts/5301fcd1/&amp;pic=https://i-blog.csdnimg.cn/direct/d8ae3ba314174e979eb534800bca9b6a.png" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl(){var e=window.location.href,t=document.createElement("input");t.setAttribute("value",e),document.body.appendChild(t),t.select(),t.setSelectionRange(0,99999),document.execCommand("copy"),document.body.removeChild(t)}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.marquis.eu.org" target="_blank">小嗷犬</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><span class="tags-punctuation"><i class="anzhiyufont anzhiyu-icon-tag"></i></span> 大模型<span class="tagsPageCount">44</span></a><a class="post-meta__box__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="tags-punctuation"><i class="anzhiyufont anzhiyu-icon-tag"></i></span> 论文笔记<span class="tagsPageCount">66</span></a><a class="post-meta__box__tags" href="/tags/%E5%BE%AE%E8%B0%83/"><span class="tags-punctuation"><i class="anzhiyufont anzhiyu-icon-tag"></i></span> 微调<span class="tagsPageCount">11</span></a></div></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/f9e2309d/"><img class="prev-cover" src="https://i-blog.csdnimg.cn/direct/fd9354b9f1774dcba4609bf7751e7340.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【论文笔记】NEFTune: Noisy Embeddings Improve Instruction Finetuning</div></div></a></div><div class="next-post pull-right"><a href="/posts/dc5a7551/"><img class="next-cover" src="https://i-blog.csdnimg.cn/direct/5dca8b5cb9904b4aa94d3a737d2e0295.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【论文笔记】Direct Preference Optimization: Your Language Model is Secretly a Reward Model</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size:1.5rem;margin-right:4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/posts/dc5a7551/" title="【论文笔记】Direct Preference Optimization: Your Language Model is Secretly a Reward Model"><img class="cover" src="https://i-blog.csdnimg.cn/direct/5dca8b5cb9904b4aa94d3a737d2e0295.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-05</div><div class="title">【论文笔记】Direct Preference Optimization: Your Language Model is Secretly a Reward Model</div></div></a></div><div><a href="/posts/655bfe1d/" title="【论文笔记】LoRA: Low-Rank Adaptation of Large Language Models"><img class="cover" src="https://i-blog.csdnimg.cn/direct/5c437c893bcd41738b1eaf3b4357c4bf.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-11-17</div><div class="title">【论文笔记】LoRA: Low-Rank Adaptation of Large Language Models</div></div></a></div><div><a href="/posts/f9e2309d/" title="【论文笔记】NEFTune: Noisy Embeddings Improve Instruction Finetuning"><img class="cover" src="https://i-blog.csdnimg.cn/direct/fd9354b9f1774dcba4609bf7751e7340.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-05</div><div class="title">【论文笔记】NEFTune: Noisy Embeddings Improve Instruction Finetuning</div></div></a></div><div><a href="/posts/6fa7adb/" title="【论文笔记】P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"><img class="cover" src="https://i-blog.csdnimg.cn/direct/954079b7f32a4cb0b0d343a8dfb24954.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-11-17</div><div class="title">【论文笔记】P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</div></div></a></div><div><a href="/posts/61cf4ab0/" title="【论文笔记】Parameter-Efficient Transfer Learning for NLP"><img class="cover" src="https://i-blog.csdnimg.cn/direct/f00f1de251164cbcaca81e2879696edd.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-11-10</div><div class="title">【论文笔记】Parameter-Efficient Transfer Learning for NLP</div></div></a></div><div><a href="/posts/af8b765f/" title="【论文笔记】PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"><img class="cover" src="https://i-blog.csdnimg.cn/direct/6c07a7a5e578400c95f2e6218f53c8ea.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-01-05</div><div class="title">【论文笔记】PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i> <span>评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)" style="display:none">匿名评论</a><a href="/privacy" style="margin-left:4px">隐私政策</a></div><div class="comment-tips" id="comment-tips"><span>✅ 你无需删除空行，直接评论以获取最佳展示效果</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src="/img/avatar/2.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"><div class="author-status"><img class="g-status" src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"></div></div><div class="author-info__description"><div style="line-height:1.38;margin:.6rem 0;text-align:justify;color:rgba(255,255,255,.8)">这有关于<b style="color:#fff">语言、算法、AI</b>相关的问题和看法，还有<b style="color:#fff">文章翻译</b>和<b style="color:#fff">分享</b>。</div><div style="line-height:1.38;margin:.6rem 0;text-align:justify;color:rgba(255,255,255,.8)">相信你可以在这里找到对你有用的<b style="color:#fff">知识</b>和<b style="color:#fff">教程</b>。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">小嗷犬</h1><div class="author-info__desc">分享技术，记录生活</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://marquis.blog.csdn.net/" target="_blank" title="CSDN"><i class="fas fa-c faa-tada"></i></a><a class="social-icon faa-parent animated-hover" href="https://www.kaggle.com/marquis03" target="_blank" title="Kaggle"><i class="fab fa-kaggle faa-tada"></i></a></div></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background:url(/img/card_wx/front.png) center center/100% no-repeat"></div><div class="back face" style="background:url(/img/card_wx/back.png) center center/100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">基本信息</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">2.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LongLoRA"><span class="toc-number">3.</span> <span class="toc-text">LongLoRA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">3.1.</span> <span class="toc-text">背景</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer"><span class="toc-number">3.1.1.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Low-rank-Adaptation"><span class="toc-number">3.1.2.</span> <span class="toc-text">Low-rank Adaptation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shifted-Sparse-Attention"><span class="toc-number">3.2.</span> <span class="toc-text">Shifted Sparse Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Pilot-Study"><span class="toc-number">3.2.1.</span> <span class="toc-text">Pilot Study</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Consistency-to-Full-Attention"><span class="toc-number">3.2.2.</span> <span class="toc-text">Consistency to Full Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Easy-Implementation"><span class="toc-number">3.2.3.</span> <span class="toc-text">Easy Implementation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Improved-LoRA-for-Long-Context"><span class="toc-number">3.3.</span> <span class="toc-text">Improved LoRA for Long Context</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.1.</span> <span class="toc-text">主实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.2.</span> <span class="toc-text">消融实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/6062c529/" title="【论文笔记】Sign Language Video Retrieval with Free-Form Textual Queries"><img src="https://i-blog.csdnimg.cn/direct/8440705e97cd474ea81703fa5d0afbae.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="【论文笔记】Sign Language Video Retrieval with Free-Form Textual Queries"></a><div class="content"><a class="title" href="/posts/6062c529/" title="【论文笔记】Sign Language Video Retrieval with Free-Form Textual Queries">【论文笔记】Sign Language Video Retrieval with Free-Form Textual Queries</a><time datetime="2025-01-12T09:51:53.000Z" title="发表于 2025-01-12 17:51:53">2025-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/af8b765f/" title="【论文笔记】PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"><img src="https://i-blog.csdnimg.cn/direct/6c07a7a5e578400c95f2e6218f53c8ea.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="【论文笔记】PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models"></a><div class="content"><a class="title" href="/posts/af8b765f/" title="【论文笔记】PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models">【论文笔记】PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a><time datetime="2025-01-05T14:30:30.000Z" title="发表于 2025-01-05 22:30:30">2025-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/dc5a7551/" title="【论文笔记】Direct Preference Optimization: Your Language Model is Secretly a Reward Model"><img src="https://i-blog.csdnimg.cn/direct/5dca8b5cb9904b4aa94d3a737d2e0295.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="【论文笔记】Direct Preference Optimization: Your Language Model is Secretly a Reward Model"></a><div class="content"><a class="title" href="/posts/dc5a7551/" title="【论文笔记】Direct Preference Optimization: Your Language Model is Secretly a Reward Model">【论文笔记】Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a><time datetime="2025-01-05T13:44:55.000Z" title="发表于 2025-01-05 21:44:55">2025-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/5301fcd1/" title="【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"><img src="https://i-blog.csdnimg.cn/direct/d8ae3ba314174e979eb534800bca9b6a.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"></a><div class="content"><a class="title" href="/posts/5301fcd1/" title="【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models">【论文笔记】LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</a><time datetime="2025-01-05T13:00:39.000Z" title="发表于 2025-01-05 21:00:39">2025-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/f9e2309d/" title="【论文笔记】NEFTune: Noisy Embeddings Improve Instruction Finetuning"><img src="https://i-blog.csdnimg.cn/direct/fd9354b9f1774dcba4609bf7751e7340.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="【论文笔记】NEFTune: Noisy Embeddings Improve Instruction Finetuning"></a><div class="content"><a class="title" href="/posts/f9e2309d/" title="【论文笔记】NEFTune: Noisy Embeddings Improve Instruction Finetuning">【论文笔记】NEFTune: Noisy Embeddings Improve Instruction Finetuning</a><time datetime="2025-01-05T12:17:40.000Z" title="发表于 2025-01-05 20:17:40">2025-01-05</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" target="_blank" rel="noopener" href="https://marquis.blog.csdn.net/" title="CSDN"><i class="fas fa-c"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://www.kaggle.com/marquis03" title="Kaggle"><i class="fab fa-kaggle"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://space.bilibili.com/97020105" title="BiliBili"><i class="fab fa-bilibili"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0,500)" src="/img/doge.gif" size="50px"><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/Marquis03" title="Github"><i class="fab fa-github"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://qm.qq.com/q/kvSr7oUR6U" title="QQ"><i class="fab fa-qq"></i></a><a class="deal_link" href="mailto:marquis128@foxmail.com" title="Email"><i class="fas fa-envelope"></i></a></div><div id="workboard"><img class="workSituationImg boardsign" src="/img/footer/小嗷犬-上班摸鱼中-007ACC.svg" alt="距离月入30k也就还差一个大佬带我~" title="距离月入30k也就还差一个大佬带我~"><div id="runtimeTextTip"></div></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="51la统计" target="_blank" rel="noopener" href="https://v6.51.la/">51la统计</a><a class="footer-item" title="开往" target="_blank" rel="noopener" href="https://github.com/travellings-link/travellings">开往</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="即刻短文" href="/essay/">即刻短文</a><a class="footer-item" title="网站收藏" href="/favorites/">网站收藏</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy/">隐私协议</a><a class="footer-item" title="Cookies" href="/cookies/">Cookies</a><a class="footer-item" title="版权协议" href="/copyright/">版权协议</a></div></div><div class="footer-group"><div class="footer-title-group"><div class="footer-title">友链</div><a class="random-friends-btn" id="footer-random-friends-btn" href="javascript:addFriendLinksInFooter();" title="换一批友情链接"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i></a></div><div class="footer-links" id="friend-links-in-footer"></div></div></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="本站框架为Hexo" title="本站框架为Hexo"><img src="/img/footer/Frame-Hexo-blue.svg" alt="本站框架为Hexo"></a><a class="github-badge" target="_blank" href="https://docs.anheyu.com/" style="margin-inline:5px" data-title="本站主题采用AnZhiYu" title="本站主题采用AnZhiYu"><img src="/img/footer/Theme-AnZhiYu-2E67D3.svg" alt="本站主题采用AnZhiYu"></a><a class="github-badge" target="_blank" href="https://www.netlify.com/" style="margin-inline:5px" data-title="本站部署于Netlify" title="本站部署于Netlify"><img src="/img/footer/Hosted-Netlify-00C7B7.svg" alt="本站部署于Netlify"></a><a class="github-badge" target="_blank" href="https://icp.gov.moe/?keyword=20222350" style="margin-inline:5px" data-title="本站已加入萌ICP豪华套餐，萌ICP备20222350号" title="本站已加入萌ICP豪华套餐，萌ICP备20222350号"><img src="/img/footer/萌ICP备-20222350-fe1384.svg" alt="本站已加入萌ICP豪华套餐，萌ICP备20222350号"></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站源码托管于Github" title="本站源码托管于Github"><img src="/img/footer/Source-Github-d021d6.svg" alt="本站源码托管于Github"></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src="/img/footer/Copyright-BY--NC--SA 4.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2023 - 2025 By <a class="footer-bar-link" href="/" title="小嗷犬" target="_blank">小嗷犬</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" href="/atom.xml" title="RSS订阅">RSS订阅</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">186</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">3</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://marquis.eu.org/" title="个人主页"><img class="back-menu-item-icon" src="/img/avatar/1.png" alt="个人主页"><span class="back-menu-item-text">个人主页</span></a><a class="back-menu-item" href="https://blog.marquis.eu.org/" title="个人博客"><img class="back-menu-item-icon" src="/img/avatar/2.png" alt="个人博客"><span class="back-menu-item-text">个人博客</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size:.9em"></i> <span>归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size:.9em"></i> <span>分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size:.9em"></i> <span>标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/charts/"><i class="fas fa-chart-bar faa-tada"></i> <span>统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=8947107806&amp;server=tencent"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size:.9em"></i> <span>音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size:.9em"></i> <span>相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/favorites/"><i class="fas fa-star faa-tada"></i> <span>收藏夹</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/equipment/"><i class="anzhiyufont anzhiyu-icon-dice-d20 faa-tada" style="font-size:.9em"></i> <span>装备栏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size:.9em"></i> <span>友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size:.9em"></i> <span>留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span>关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size:.9em"></i> <span>关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size:.9em"></i> <span>闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size:.9em"></i> <span>随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/C/" style="font-size:.88rem">C<sup>1</sup></a><a href="/tags/CPP/" style="font-size:.88rem">CPP<sup>5</sup></a><a href="/tags/LaTeX/" style="font-size:.88rem">LaTeX<sup>4</sup></a><a href="/tags/Linux/" style="font-size:.88rem">Linux<sup>1</sup></a><a href="/tags/MATLAB/" style="font-size:.88rem;font-weight:500;color:var(--anzhiyu-lighttext)">MATLAB<sup>28</sup></a><a href="/tags/Markdown/" style="font-size:.88rem">Markdown<sup>5</sup></a><a href="/tags/Python/" style="font-size:.88rem;font-weight:500;color:var(--anzhiyu-lighttext)">Python<sup>43</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size:.88rem">前端<sup>1</sup></a><a href="/tags/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" style="font-size:.88rem">动作识别<sup>2</sup></a><a href="/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1/" style="font-size:.88rem">多任务<sup>1</sup></a><a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size:.88rem">多模态<sup>41</sup></a><a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="font-size:.88rem">大模型<sup>44</sup></a><a href="/tags/%E5%B0%91%E6%A0%B7%E6%9C%AC/" style="font-size:.88rem">少样本<sup>4</sup></a><a href="/tags/%E5%BE%AE%E8%B0%83/" style="font-size:.88rem">微调<sup>11</sup></a><a href="/tags/%E6%89%8B%E8%AF%AD%E7%BF%BB%E8%AF%91/" style="font-size:.88rem">手语翻译<sup>21</sup></a><a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size:.88rem">操作系统<sup>3</sup></a><a href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" style="font-size:.88rem">数学建模<sup>20</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size:.88rem">数据分析<sup>5</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/" style="font-size:.88rem">数据可视化<sup>2</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size:.88rem">机器学习<sup>8</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size:.88rem">深度学习<sup>19</sup></a><a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size:.88rem">爬虫<sup>5</sup></a><a href="/tags/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" style="font-size:.88rem">程序设计<sup>11</sup></a><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size:.88rem">论文笔记<sup>66</sup></a></div></div><hr></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><a id="switch-commentBarrage" href="javascript:anzhiyu.switchCommentBarrage();" title="开关弹幕"><i class="anzhiyufont anzhiyu-icon-danmu"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8947107806" server="tencent" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.5"></meting-js></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"><a class="tag-list" href="/tags/Python" title="Python">Python</a><a class="tag-list" href="/tags/MATLAB" title="MATLAB">MATLAB</a></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size:1rem"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8947107806&quot;, &quot;_blank&quot;);" style="display:none"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>var meting_api="https://api.injahow.cn/meting/?server=:server&type=:type&id=:id&auth=:auth&r=:r"</script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script async src="/anzhiyu/random.js"></script><script async>!function(){var e,t,n,r,o,i=new Date("01/16/2023 00:00:00"),a=new Date;setInterval((()=>{!function(){a=new Date,o=a.getHours();var l=(a-i)/1e3/60/60/24;e=Math.floor(l);var u=(a-i)/1e3/60/60-24*e;t=Math.floor(u),1==String(t).length&&(t="0"+t);var c=(a-i)/1e3/60-1440*e-60*t;n=Math.floor(c),1==String(n).length&&(n="0"+n);var s=(a-i)/1e3-86400*e-3600*t-60*n;r=Math.round(s),1==String(r).length&&(r="0"+r)}(),function(){if(!document.getElementById("footer"))return;let i="";if(o<18&&o>=9)i=`本站居然运行了 ${e} 天<span id='runtime'> ${t} 小时 ${n} 分 ${r} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;else{let o=document.querySelector("#workboard .workSituationImg");null!=o&&(o.src="/img/footer/小嗷犬-下班摆烂啦-2F2625.svg",o.title="下班了就该开开心心的玩耍，嘿嘿~",o.alt="下班了就该开开心心的玩耍，嘿嘿~"),i=`本站居然运行了 ${e} 天<span id='runtime'> ${t} 小时 ${n} 分 ${r} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`}document.getElementById("runtimeTextTip")&&(document.getElementById("runtimeTextTip").innerHTML=i)}()}),1e3)}()</script><script src="https://cdn.cbd.int/algoliasearch@4.18.0/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.cbd.int/instantsearch.js@4.60.0/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><link rel="stylesheet" href="https://cdn.cbd.int/katex@0.16.0/dist/katex.min.css"><script src="https://cdn.cbd.int/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script>document.querySelectorAll("#article-container span.katex-display").forEach((a=>{anzhiyu.wrap(a,"div",{class:"katex-wrap"})}))</script><script>(()=>{const t=()=>{"object"==typeof twikoo?setTimeout(o,0):getScript("https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js").then(o)},o=()=>{twikoo.init(Object.assign({el:"#twikoo-wrap",envId:"https://twikoo.marquis.eu.org/",region:"",onCommentLoaded:()=>{anzhiyu.loadLightbox(document.querySelectorAll("#twikoo .tk-content img:not(.tk-owo-emotion)"))}},null)),GLOBAL_CONFIG_SITE.isPost&&(()=>{const t=document.getElementById("twikoo-count");t&&twikoo.getCommentsCount({envId:"https://twikoo.marquis.eu.org/",region:"",urls:[window.location.pathname],includeReply:!1}).then((o=>{t.textContent=o[0].count})).catch((t=>{console.error(t)}))})()};t()})()</script><input type="hidden" name="page-type" id="page-type" value="post"><script async src="/js/anzhiyu/comment_barrage.js"></script></div><script>window.addEventListener("load",(()=>{const e=e=>{let t="";if(e.length)for(let n=0;n<e.length;n++){t+="<div class='aside-list-item'>";{const a="data-lazy-src";t+=`<a href='${e[n].url}' class='thumbnail'><img ${a}='${e[n].avatar}' alt='${e[n].nick}'><div class='name'><span>${e[n].nick} </span></div></a>`}t+=`<div class='content'>\n        <a class='comment' href='${e[n].url}' title='${e[n].content}'>${e[n].content}</a>\n        <time datetime="${e[n].date}">${anzhiyu.diffDate(e[n].date,!0)}</time></div>\n        </div>`}else t+="没有评论";let n=document.querySelector("#card-newest-comments .aside-list");n&&(n.innerHTML=t),window.lazyLoadInstance&&window.lazyLoadInstance.update(),window.pjax&&window.pjax.refresh(n)},t=()=>{if(document.querySelector("#card-newest-comments .aside-list")){const t=saveToLocal.get("twikoo-newest-comments");t?e(JSON.parse(t)):(()=>{const t=()=>{twikoo.getRecentComments({envId:"https://twikoo.marquis.eu.org/",region:"",pageSize:6,includeReply:!0}).then((function(t){const n=t.map((e=>{return{content:(t=e.comment,""===t||(t=(t=(t=(t=t.replace(/<img.*?src="(.*?)"?[^\>]+>/gi,"[图片]")).replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi,"[链接]")).replace(/<pre><code>.*?<\/pre>/gi,"[代码]")).replace(/<[^>]+>/g,"")).length>150&&(t=t.substring(0,150)+"..."),t),avatar:e.avatar,nick:e.nick,url:e.url+"#"+e.id,date:new Date(e.created).toISOString()};var t}));saveToLocal.set("twikoo-newest-comments",JSON.stringify(n),10/1440),e(n)})).catch((function(e){document.querySelector("#card-newest-comments .aside-list").textContent="无法获取评论，请确认相关配置是否正确"}))};"object"==typeof twikoo?t():getScript("https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js").then(t)})()}};t(),document.addEventListener("pjax:complete",t)}))</script><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script defer src="/js/console.js"></script><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script id="click-show-text" src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法制,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="false" async></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload='this.media="all"'><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors=['meta[property="og:image"]','meta[property="og:title"]','meta[property="og:url"]','meta[property="og:type"]','meta[property="og:site_name"]','meta[property="og:description"]',"head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",(function(){if(anzhiyu.removeGlobalFnEvent("pjax"),anzhiyu.removeGlobalFnEvent("themeChange"),document.getElementById("rightside").classList.remove("rightside-show"),window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode")})),document.addEventListener("pjax:complete",(function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)})),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll()})),document.addEventListener("pjax:error",(e=>{404===e.request.status&&pjax.loadUrl("/404.html")}))</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>